{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            images.append(filename)\n",
    "    return images\n",
    "\n",
    "def pre_processing(data):\n",
    "    label_ = []\n",
    "    path_ = []\n",
    "    for img in data:\n",
    "        path_.append(path_to_train+img)\n",
    "    for img in data:\n",
    "        id_ = img.split(\".\")[0]\n",
    "        breed = labels.loc[labels.id == id_, \"breed\"].values[0]\n",
    "        label_.append(breed_dict[breed])\n",
    "    return path_, label_\n",
    "\n",
    "def parser(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image_resized = tf.image.resize_images(image_decoded,image_size)/255.\n",
    "    return image_resized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = \"all/train/\"\n",
    "\n",
    "data_full = load_images(path_to_train)\n",
    "\n",
    "# Setting seed and random shuffling\n",
    "random.seed(100)\n",
    "random.shuffle(data_full)\n",
    "\n",
    "# Splitting in train and dev set\n",
    "train = data_full[:9982]\n",
    "train.sort()\n",
    "dev = data_full[9982:]\n",
    "dev.sort()\n",
    "\n",
    "# Creating labels\n",
    "labels = pd.read_csv(\"all/labels.csv\")\n",
    "breeds = list(labels.breed.unique())\n",
    "breed_dict = {}\n",
    "for idx, breed in enumerate(breeds):\n",
    "    breed_dict[breed] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, train_label = pre_processing(train)\n",
    "dev_path, dev_label = pre_processing(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vector of filenames.\n",
    "filenames_train = tf.constant(train_path)\n",
    "label_train = tf.one_hot(train_label,120)\n",
    "\n",
    "# `labels[i]` is the label for the image in `filenames[i].\n",
    "\n",
    "filenames_dev = tf.constant(dev_path)\n",
    "labels_dev = tf.one_hot(dev_label, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (160,160)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "step_size = 10\n",
    "num_batches = int(len(train)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((filenames_train,label_train))\n",
    "dataset = dataset.map(parser)\n",
    "dataset = dataset.repeat(num_epochs).shuffle(buffer_size=100)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "x, y = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Model\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=x ,filters=32, kernel_size=[3,3], padding=\"same\", strides=1, activation=tf.nn.relu)\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2], strides=2)\n",
    "\n",
    "bnorm1 = tf.layers.batch_normalization(inputs=pool1)\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=bnorm1 ,filters=64, kernel_size=[3,3], padding=\"same\", strides=1, activation=tf.nn.relu)\n",
    "\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], strides=2)\n",
    "\n",
    "bnorm2 = tf.layers.batch_normalization(inputs=pool2)\n",
    "\n",
    "conv3 = tf.layers.conv2d(inputs=bnorm2 ,filters=128, kernel_size=[3,3], padding=\"same\", strides=1, activation=tf.nn.relu)\n",
    "\n",
    "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2,2], strides=2)\n",
    "\n",
    "bnorm3 = tf.layers.batch_normalization(inputs=pool3)\n",
    "\n",
    "flat_layer = tf.contrib.layers.flatten(bnorm3)\n",
    "\n",
    "fc1 = tf.layers.dense(inputs=flat_layer, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "bnorm4 = tf.layers.batch_normalization(inputs=fc1)\n",
    "\n",
    "fc2 = tf.layers.dense(inputs=bnorm4, units=512, activation=tf.nn.relu)\n",
    "\n",
    "bnorm5 = tf.layers.batch_normalization(inputs=fc2)\n",
    "\n",
    "logits = tf.layers.dense(inputs=bnorm5, units=120)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        for step in range(1,num_batches+1):\n",
    "          # Training and calculating loss for the batch\n",
    "            _, batch_loss = sess.run((train_op,loss))\n",
    "\n",
    "            #log_loss_train.append(\"Epoch\"+str(epoch)+\"Step\"+str(step)+\"_\"+str(batch_loss))\n",
    "\n",
    "            if (step%step_size == 0 or step==1) and (step%100 != 0):\n",
    "\n",
    "                batch_acc = sess.run(accuracy)\n",
    "\n",
    "                print(\"Epoch \" + str(epoch) + \"::Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(batch_loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(batch_acc))\n",
    "\n",
    "#             if step%100 == 0 or step==num_batches:\n",
    "\n",
    "#                 test_loss, test_acc = testing()\n",
    "#                 #log_loss_test.append(\"Epoch\"+str(epoch)+\"Step\"+str(step)+\"_\"+str(test_loss))\n",
    "\n",
    "#                 print(\"Epoch \" + str(epoch) + \"::Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "#                   \"{:.4f}\".format(batch_loss) + \", Training Accuracy= \" + \\\n",
    "#                   \"{:.3f}\".format(batch_acc) + \", Testing Loss= \" + \\\n",
    "#                   \"{:.4f}\".format(test_loss) + \", Testing Accuracy= \" + \\\n",
    "#                   \"{:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
